{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conflab.data_loaders.pose import ConflabPoseExtractor, ConflabToKinetics\n",
    "from conflab.data_loaders.accel import ConflabAccelExtractor\n",
    "from conflab.data_loaders.person import ConflabDataset, ConflabSubset, ConflabLabelExtractor\n",
    "from conflab.baselines.speaking_status.pose.feeders.feeder import Feeder\n",
    "from conflab.constants import conflab_pose_path, midge_data_path, conflab_speaking_status_path\n",
    "from utils import count_params, import_class, get_metrics\n",
    "from trainer import Trainer\n",
    "from utils import count_params, import_class, init_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    def worker_seed_fn(worker_id):\n",
    "        # give workers different seeds\n",
    "        return init_seed(args['seed'] + worker_id + 1)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=Feeder(**args['train_feeder_args']),\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=args['num_worker'],\n",
    "        drop_last=True,\n",
    "        worker_init_fn=worker_seed_fn)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=Feeder(**args['test_feeder_args']),\n",
    "        batch_size=args['test_batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=args['num_worker'],\n",
    "        drop_last=False,\n",
    "        worker_init_fn=worker_seed_fn)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = yaml.safe_load(open('./config/defaults.yaml', 'r'))\n",
    "config = yaml.safe_load(open('./config/conflab/train_joint.yaml', 'r'))\n",
    "config = {**defaults, **config}\n",
    "config['phase'] = 'train'\n",
    "config['work_dir'] = 'transfer_learning/conflab/joint'\n",
    "config['weights'] = 'pretrained-models/kinetics-joint.pt'\n",
    "config['ignore_weights'] = ['fc.weight', 'fc.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 180, 18, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir transfer_learning/conflab/joint/trainlogs already exists\n",
      "Dir removed: transfer_learning/conflab/joint/trainlogs\n",
      "Cannot parse global_step from model weights filename\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(BS 64) loss: 1.7460:  57%|█████▋    | 77/136 [00:57<00:43,  1.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m load_data(config)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(config)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(train_loader, test_loader)\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py:256\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dl, eval_dl)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=253'>254</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39mstart_epoch\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39mnum_epoch\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=254'>255</a>\u001b[0m     save_model \u001b[39m=\u001b[39m ((epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39msave_interval\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39mnum_epoch\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=255'>256</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(epoch, train_dl, save_model\u001b[39m=\u001b[39;49msave_model)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=256'>257</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(epoch, eval_dl, save_score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39msave_score\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=258'>259</a>\u001b[0m num_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py:304\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, epoch, loader, save_model)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=300'>301</a>\u001b[0m batch_data, batch_label \u001b[39m=\u001b[39m data[left:right], label[left:right]\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=302'>303</a>\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=303'>304</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch_data)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=305'>306</a>\u001b[0m     output, l1 \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py:163\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=159'>160</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgcn1(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn3d1(x), inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=160'>161</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtcn1(x)\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=162'>163</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgcn2(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcn3d2(x), inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=163'>164</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtcn2(x)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=165'>166</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgcn3(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn3d3(x), inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py:100\u001b[0m, in \u001b[0;36mMultiWindow_MS_G3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=97'>98</a>\u001b[0m out_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=98'>99</a>\u001b[0m \u001b[39mfor\u001b[39;00m gcn3d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn3d:\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=99'>100</a>\u001b[0m     out_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m gcn3d(x)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=100'>101</a>\u001b[0m \u001b[39m# no activation\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out_sum\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py:61\u001b[0m, in \u001b[0;36mMS_G3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=58'>59</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min1x1(x)\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=59'>60</a>\u001b[0m \u001b[39m# Construct temporal windows and apply MS-GCN\u001b[39;00m\n\u001b[0;32m---> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=60'>61</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcn3d(x)\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=62'>63</a>\u001b[0m \u001b[39m# Collapse the window dimension\u001b[39;00m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=63'>64</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_channels_out, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, V)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py:99\u001b[0m, in \u001b[0;36mSpatialTemporal_MS_GCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=95'>96</a>\u001b[0m N, C, T, V \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape    \u001b[39m# T = number of windows\u001b[39;00m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=97'>98</a>\u001b[0m \u001b[39m# Build graphs\u001b[39;00m\n\u001b[0;32m---> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=98'>99</a>\u001b[0m A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA_scales\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_res\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=100'>101</a>\u001b[0m \u001b[39m# Perform Graph Convolution\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=101'>102</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data(config)\n",
    "trainer = Trainer(config)\n",
    "trainer.train(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = yaml.safe_load(open('./config/defaults.yaml', 'r'))\n",
    "config = yaml.safe_load(open('./config/conflab/train_joint.yaml', 'r'))\n",
    "config = {**defaults, **config}\n",
    "config['phase'] = 'train'\n",
    "config['work_dir'] = 'conflab'\n",
    "config['weights'] = 'pretrained-models/kinetics-joint.pt'\n",
    "config['ignore_weights'] = ['fc.weight', 'fc.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format conflab dataset dict output to msg3d tuple (data, label, index) \n",
    "def collate(batch):\n",
    "    data = [e['pose'] for e in batch]\n",
    "    labels = [e['label'] for e in batch]\n",
    "    idxs = [e['index'] for e in batch]\n",
    "\n",
    "    return torch.stack(data), torch.tensor(labels), torch.tensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fold(train_ds, test_ds, deterministic=False):\n",
    "    # data loaders\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=128, shuffle=True, num_workers=4,\n",
    "        collate_fn=collate, drop_last=True)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        test_ds, batch_size=128, shuffle=False, num_workers=4,\n",
    "        collate_fn=collate, drop_last=True)\n",
    "\n",
    "    trainer = Trainer(config)\n",
    "    trainer.train(data_loader_train, data_loader_val)\n",
    "\n",
    "    return trainer.test(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_run(dataset, random_state, metrics_name='binary', deterministic=False):\n",
    "    cv_splits = KFold(n_splits=4, random_state=random_state, shuffle=True).split(range(len(ds)))\n",
    "\n",
    "    metrics = []\n",
    "    scores = []\n",
    "    for f, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        # create datasets    \n",
    "        train_ds = ConflabSubset(dataset, train_idx)\n",
    "        test_ds = ConflabSubset(dataset, test_idx)\n",
    "\n",
    "        fold_metrics, fold_scores = do_fold(train_ds, test_ds, deterministic=deterministic)\n",
    "        clear_output(wait=True)\n",
    "        metrics.append(fold_metrics)\n",
    "        scores.append(fold_scores)\n",
    "\n",
    "    return metrics, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_extractor = ConflabPoseExtractor(conflab_pose_path)\n",
    "pose_extractor.load_from_pickle('../tracks.pkl')\n",
    "label_extractor = ConflabLabelExtractor(os.path.join(conflab_speaking_status_path, 'speaking'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 544.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# make windowed examples using the pose tracks.\n",
    "examples = pose_extractor.make_examples()\n",
    "# compose the dataset\n",
    "ds = ConflabDataset(examples, {\n",
    "    'pose': pose_extractor,\n",
    "    'label': label_extractor\n",
    "}, transform=ConflabToKinetics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir is transfer_learning/conflab/joint/trainlogs\n",
      "log_dir transfer_learning/conflab/joint/trainlogs already exists\n",
      "Dir removed: transfer_learning/conflab/joint/trainlogs\n",
      "Cannot parse global_step from model weights filename\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(BS 64) loss: 0.6167:  22%|██▏       | 54/251 [00:36<02:11,  1.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000018vscode-remote?line=0'>1</a>\u001b[0m seed\u001b[39m=\u001b[39m\u001b[39m22\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000018vscode-remote?line=1'>2</a>\u001b[0m pl\u001b[39m.\u001b[39mutilities\u001b[39m.\u001b[39mseed\u001b[39m.\u001b[39mseed_everything(seed, workers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000018vscode-remote?line=2'>3</a>\u001b[0m metrics, scores \u001b[39m=\u001b[39m do_run(ds, random_state\u001b[39m=\u001b[39;49mseed, metrics_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbinary\u001b[39;49m\u001b[39m'\u001b[39;49m, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb Cell 16'\u001b[0m in \u001b[0;36mdo_run\u001b[0;34m(dataset, random_state, metrics_name, deterministic)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000015vscode-remote?line=7'>8</a>\u001b[0m train_ds \u001b[39m=\u001b[39m ConflabSubset(dataset, train_idx)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000015vscode-remote?line=8'>9</a>\u001b[0m test_ds \u001b[39m=\u001b[39m ConflabSubset(dataset, test_idx)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000015vscode-remote?line=10'>11</a>\u001b[0m fold_metrics, fold_scores \u001b[39m=\u001b[39m do_fold(train_ds, test_ds, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000015vscode-remote?line=11'>12</a>\u001b[0m clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000015vscode-remote?line=12'>13</a>\u001b[0m metrics\u001b[39m.\u001b[39mappend(fold_metrics)\n",
      "\u001b[1;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb Cell 15'\u001b[0m in \u001b[0;36mdo_fold\u001b[0;34m(train_ds, test_ds, deterministic)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000014vscode-remote?line=5'>6</a>\u001b[0m data_loader_val \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000014vscode-remote?line=6'>7</a>\u001b[0m     test_ds, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000014vscode-remote?line=7'>8</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mcollate, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000014vscode-remote?line=9'>10</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(config)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000014vscode-remote?line=10'>11</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(data_loader_train, data_loader_val)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/run.ipynb#ch0000014vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mtest(data_loader_val)\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py:259\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dl, eval_dl)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=256'>257</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39mstart_epoch\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39mnum_epoch\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=257'>258</a>\u001b[0m     save_model \u001b[39m=\u001b[39m ((epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39msave_interval\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39mnum_epoch\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=258'>259</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(epoch, train_dl, save_model\u001b[39m=\u001b[39;49msave_model)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=259'>260</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval(epoch, eval_dl, save_score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg[\u001b[39m'\u001b[39m\u001b[39msave_score\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=261'>262</a>\u001b[0m num_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py:307\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, epoch, loader, save_model)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=303'>304</a>\u001b[0m batch_data, batch_label \u001b[39m=\u001b[39m data[left:right], label[left:right]\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=305'>306</a>\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=306'>307</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch_data)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=307'>308</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/trainer.py?line=308'>309</a>\u001b[0m     output, l1 \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py:163\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=159'>160</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgcn1(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn3d1(x), inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=160'>161</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtcn1(x)\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=162'>163</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgcn2(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcn3d2(x), inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=163'>164</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtcn2(x)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=165'>166</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgcn3(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn3d3(x), inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py:100\u001b[0m, in \u001b[0;36mMultiWindow_MS_G3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=97'>98</a>\u001b[0m out_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=98'>99</a>\u001b[0m \u001b[39mfor\u001b[39;00m gcn3d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn3d:\n\u001b[0;32m--> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=99'>100</a>\u001b[0m     out_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m gcn3d(x)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=100'>101</a>\u001b[0m \u001b[39m# no activation\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out_sum\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py:61\u001b[0m, in \u001b[0;36mMS_G3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=58'>59</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min1x1(x)\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=59'>60</a>\u001b[0m \u001b[39m# Construct temporal windows and apply MS-GCN\u001b[39;00m\n\u001b[0;32m---> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=60'>61</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcn3d(x)\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=62'>63</a>\u001b[0m \u001b[39m# Collapse the window dimension\u001b[39;00m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/msg3d.py?line=63'>64</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_channels_out, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, V)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jose/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py:99\u001b[0m, in \u001b[0;36mSpatialTemporal_MS_GCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=95'>96</a>\u001b[0m N, C, T, V \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape    \u001b[39m# T = number of windows\u001b[39;00m\n\u001b[1;32m     <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=97'>98</a>\u001b[0m \u001b[39m# Build graphs\u001b[39;00m\n\u001b[0;32m---> <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=98'>99</a>\u001b[0m A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA_scales\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_res\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=100'>101</a>\u001b[0m \u001b[39m# Perform Graph Convolution\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/c/Users/Jose/Documents/furnace/conflab/baselines/speaking_status/pose/model/ms_gtcn.py?line=101'>102</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed=22\n",
    "pl.utilities.seed.seed_everything(seed, workers=True)\n",
    "metrics, scores = do_run(ds, random_state=seed, metrics_name='binary', deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.6808035714285714,\n",
       "  'loss': 1.958919610295977,\n",
       "  'auc': 0.6655879003830165},\n",
       " {'accuracy': 0.6138392857142857,\n",
       "  'loss': 2.812851599284581,\n",
       "  'auc': 0.6278296578440496},\n",
       " {'accuracy': 0.6450892857142857,\n",
       "  'loss': 1.9170774562018258,\n",
       "  'auc': 0.6392705881043416},\n",
       " {'accuracy': 0.6183035714285714,\n",
       "  'loss': 3.0535796540124074,\n",
       "  'auc': 0.5869259331639632}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
